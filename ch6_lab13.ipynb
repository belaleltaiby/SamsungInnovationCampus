{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c7fccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 10:01:17,519 WARN sources.TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "postsDF = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", \"44444\").load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b62e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "aPostDF = (postsDF\n",
    "    .withColumn(\"id\", split(postsDF.value, \",\")[0].cast(\"integer\"))\n",
    "    .withColumn(\"author_id\", split(postsDF.value, \",\")[1].cast(\"integer\"))\n",
    "    .withColumn(\"title\", split(postsDF.value, \",\")[2])\n",
    "    .withColumn(\"description\", split(postsDF.value, \",\")[3])\n",
    "    .withColumn(\"content\", split(postsDF.value, \",\")[4])\n",
    "    .withColumn(\"date\", split(postsDF.value, \",\")[5])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5540871",
   "metadata": {},
   "outputs": [],
   "source": [
    "myOutDF = aPostDF.select(\"author_id\",aPostDF.title[0:10].alias(\"Post_Title\"),\"date\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c1674aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 10:13:13,505 WARN streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a38a9a0a-2d69-4701-8a16-7285201e3203. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "2024-09-24 10:13:14,667 ERROR streaming.MicroBatchExecution: Query [id = 649948e8-8903-48c7-aed2-4c74ad0d5022, runId = 119f4514-ee99-4e48-afae-20ea817a9d49] terminated with error\n",
      "java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:476)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:218)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:200)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:394)\n",
      "\tat java.net.Socket.connect(Socket.java:606)\n",
      "\tat java.net.Socket.connect(Socket.java:555)\n",
      "\tat java.net.Socket.<init>(Socket.java:451)\n",
      "\tat java.net.Socket.<init>(Socket.java:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.TextSocketMicroBatchStream.initialize(TextSocketMicroBatchStream.scala:71)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.TextSocketMicroBatchStream.planInputPartitions(TextSocketMicroBatchStream.scala:117)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.partitions$lzycompute(MicroBatchScanExec.scala:44)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.partitions(MicroBatchScanExec.scala:44)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:87)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:86)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.supportsColumnar(MicroBatchScanExec.scala:29)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:120)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:162)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:162)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:162)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:391)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:104)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:117)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:117)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$14(MicroBatchExecution.scala:577)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:567)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:226)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:194)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:334)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "Connection refused (Connection refused)\n=== Streaming Query ===\nIdentifier: [id = 649948e8-8903-48c7-aed2-4c74ad0d5022, runId = 119f4514-ee99-4e48-afae-20ea817a9d49]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {TextSocketV2[host: localhost, port: 44444]: -1}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource ConsoleWriter[numRows=20, truncate=false]\n+- Project [author_id#162, substring(title#166, 0, 10) AS Post_Title#192, date#184]\n   +- Project [value#157, id#159, author_id#162, title#166, description#171, content#177, split(value#157, ,, -1)[5] AS date#184]\n      +- Project [value#157, id#159, author_id#162, title#166, description#171, split(value#157, ,, -1)[4] AS content#177]\n         +- Project [value#157, id#159, author_id#162, title#166, split(value#157, ,, -1)[3] AS description#171]\n            +- Project [value#157, id#159, author_id#162, split(value#157, ,, -1)[2] AS title#166]\n               +- Project [value#157, id#159, cast(split(value#157, ,, -1)[1] as int) AS author_id#162]\n                  +- Project [value#157, cast(split(value#157, ,, -1)[0] as int) AS id#159]\n                     +- StreamingDataSourceV2Relation [value#157], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@724208b, TextSocketV2[host: localhost, port: 44444]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5277/804447357.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmyStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark3/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark3/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark3/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Connection refused (Connection refused)\n=== Streaming Query ===\nIdentifier: [id = 649948e8-8903-48c7-aed2-4c74ad0d5022, runId = 119f4514-ee99-4e48-afae-20ea817a9d49]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {TextSocketV2[host: localhost, port: 44444]: -1}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource ConsoleWriter[numRows=20, truncate=false]\n+- Project [author_id#162, substring(title#166, 0, 10) AS Post_Title#192, date#184]\n   +- Project [value#157, id#159, author_id#162, title#166, description#171, content#177, split(value#157, ,, -1)[5] AS date#184]\n      +- Project [value#157, id#159, author_id#162, title#166, description#171, split(value#157, ,, -1)[4] AS content#177]\n         +- Project [value#157, id#159, author_id#162, title#166, split(value#157, ,, -1)[3] AS description#171]\n            +- Project [value#157, id#159, author_id#162, split(value#157, ,, -1)[2] AS title#166]\n               +- Project [value#157, id#159, cast(split(value#157, ,, -1)[1] as int) AS author_id#162]\n                  +- Project [value#157, cast(split(value#157, ,, -1)[0] as int) AS id#159]\n                     +- StreamingDataSourceV2Relation [value#157], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@724208b, TextSocketV2[host: localhost, port: 44444]\n"
     ]
    }
   ],
   "source": [
    "myStream = (\n",
    "    myOutDF.writeStream\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"2 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "myStream.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6feb16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd5cf4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a505c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
