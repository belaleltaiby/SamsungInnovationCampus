{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b401a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.classification import LogisticRegression \n",
    "from pyspark.ml.feature import HashingTF, Tokenizer \n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "training = spark.createDataFrame([ \n",
    " (0, \"This is a testing for spark\", 1.0), \n",
    " (1, \"kudu ozone\", 0.0), \n",
    " (2, \"spark is in-memory engine\", 1.0), \n",
    " (3, \"hive is data warehouse\", 0.0), \n",
    " (4, \"hadoop is mapreduce for batch\", 0.0) \n",
    "], [\"id\", \"text\", \"label\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca6a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr. \n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\") \n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\") \n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001) \n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6748e34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 11:12:37,941 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2024-09-24 11:12:38,006 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(training) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a798a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------+-----+\n",
      "|id |text                         |label|\n",
      "+---+-----------------------------+-----+\n",
      "|0  |This is a testing for spark  |1.0  |\n",
      "|1  |kudu ozone                   |0.0  |\n",
      "|2  |spark is in-memory engine    |1.0  |\n",
      "|3  |hive is data warehouse       |0.0  |\n",
      "|4  |hadoop is mapreduce for batch|0.0  |\n",
      "+---+-----------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show(5,False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11da18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "Document = Row(\"id\", \"text\") \n",
    "test = sc.parallelize([(5, \"K O 1\"), \n",
    " (6, \"spark hadoop spark impala\"), \n",
    " (7, \"apache spark open-source\"), \n",
    " (8, \"spark is a platform\"), \n",
    " (9, \"Hadoop is for Big Data\")]).map(lambda x: Document(*x)).toDF() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b7d879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, K O 1) --> prob=[0.760510559377351,0.23948944062264899], prediction=0.000000\n",
      "(6, spark hadoop spark impala) --> prob=[0.09933562532409927,0.9006643746759008], prediction=1.000000\n",
      "(7, apache spark open-source) --> prob=[0.1938094405325343,0.8061905594674657], prediction=1.000000\n",
      "(8, spark is a platform) --> prob=[0.03587540471389243,0.9641245952861076], prediction=1.000000\n",
      "(9, Hadoop is for Big Data) --> prob=[0.9907289093358328,0.009271090664167203], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test documents and print columns of interest \n",
    "prediction = model.transform(test) \n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\") \n",
    "for row in selected.collect(): \n",
    " rid, text, prob, prediction = row \n",
    " print( \n",
    " \"(%d, %s) --> prob=%s, prediction=%f\" % ( \n",
    " rid, text, str(prob), prediction \n",
    " ) \n",
    " ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079671b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
